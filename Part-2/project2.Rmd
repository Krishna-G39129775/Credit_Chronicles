---
title: ""
author: "Saikrishna Paila"
date: "10/26/2023"
output:
    rmdformats::readthedown:
      toc_float: true
      toc_depth: 3
      number_sections: false
      code_folding: hide
---
```{r}
library(caret)
library(nnet)
library(rpart)
library(randomForest)
library(gbm)
library(e1071)
library(pROC)
```

```{r}
Credit_card_churn <- read.csv('credit_card1.csv')
str(Credit_card_churn)
```

```{r}
# Load the data
data <- read.csv('credit_card.csv')
# Drop specific columns
data$Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_1 <- NULL
data$Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_2 <- NULL

```

```{r}
str(Credit_card_churn)
summary(Credit_card_churn)
```

```{r}
print(head(Credit_card_churn,5))
```

```{r}
table(Credit_card_churn$Attrition_Flag, Credit_card_churn$Customer_Age)
table(Credit_card_churn$Attrition_Flag, Credit_card_churn$Gender)

```

```{r}
ggplot(Credit_card_churn, aes(x=Attrition_Flag,
                  y= prop.table(stat(count)),
                  fill= factor(Gender),
                  label= scales::percent(prop.table(stat(count))))) +
  geom_bar(position = position_dodge())+
  geom_text(stat="count",
            position = position_dodge(.9),
            vjust= -0.5, size=3)+
  scale_y_continuous(labels = scales::percent)+
  labs(title = "Attrition by Gender",
       x= "Attrition status",
       y="Count")+
  theme_classic()+
  scale_fill_discrete(
    name="Gender",
    breaks=c("M", "F"),
    labels=c("Male", "Female" )
  )

ggplot(Credit_card_churn, aes(x=Attrition_Flag,
                  y= prop.table(stat(count)),
                  fill= factor(Card_Category),
                  label= scales::percent(prop.table(stat(count))))) +
  geom_bar(position = position_dodge())+
  geom_text(stat="count",
            position = position_dodge(.9),
            vjust= -0.5, size=3)+
  scale_y_continuous(labels = scales::percent)+
  labs(title = "Attrition by Card Category",
       x= "Attrition status",
       y="Count")+
  theme_classic()

ggplot(Credit_card_churn, aes(x=Attrition_Flag,
                  y= prop.table(stat(count)),
                  fill= factor(Income_Category),
                  label= scales::percent(prop.table(stat(count))))) +
  geom_bar(position = position_dodge())+
  geom_text(stat="count",
            position = position_dodge(.9),
            vjust= -0.5, size=3)+
  scale_y_continuous(labels = scales::percent)+
  labs(title = "Attrition by Income Category",
       x= "Attrition status",
       y="Count")+
  theme_classic()


```

```{r}
#Converting all features to categorical data
Credit_card_churn[sapply(Credit_card_churn, is.character)]<- lapply(Credit_card_churn[sapply(Credit_card_churn, is.character)], as.factor)
```


```{r}
str(Credit_card_churn)
summary(Credit_card_churn)
```

```{r}
#Splitting the regular dataset

# Assuming you have loaded the necessary data as 'Credit_card_churn'

# Convert all features to categorical data
Credit_card_churn[sapply(Credit_card_churn, is.character)] <- lapply(Credit_card_churn[sapply(Credit_card_churn, is.character)], as.factor)

# Split the dataset into training and testing sets
set.seed(123)
intrain_reg <- createDataPartition(Credit_card_churn$Attrition_Flag, p = 0.80, list = FALSE)
training_reg <- Credit_card_churn[intrain_reg, ]
testing_reg <- Credit_card_churn[-intrain_reg, ]
dim(training_reg); dim(testing_reg)
#summary(training_reg)
#summary(testing_reg)
```

```{r}
# Fit Random Forest model
random_forest <- randomForest(Attrition_Flag ~ ., ntree = 500, family = "binomial", data = training_reg)

# Print summary of the random forest model
print(summary(random_forest))
print(random_forest)

# Make predictions on the testing set
rf_pred <- predict(random_forest, testing_reg)

# Calculate confusion matrix
conf_matrix_rf <- caret::confusionMatrix(rf_pred, testing_reg$Attrition_Flag)

# Extract overall accuracy from confusion matrix
ran_accuracy <- conf_matrix_rf$overall["Accuracy"]
cat("Random Forest Accuracy:", ran_accuracy, "\n")

# Print precision, recall, TP, FP, FN, TN
precision <- conf_matrix_rf$byClass["Precision"]
recall <- conf_matrix_rf$byClass["Recall"]
f1_score <- 2 * (precision * recall) / (precision + recall)  # Calculate F1 Score
tp <- conf_matrix_rf$table[2, 2]
fp <- conf_matrix_rf$table[1, 2]
fn <- conf_matrix_rf$table[2, 1]
tn <- conf_matrix_rf$table[1, 1]

cat("Precision:", precision, "\n")
cat("Recall:", recall, "\n")
cat("F1 Score:", f1_score, "\n")  # Print F1 Score
cat("True Positives:", tp, "\n")
cat("False Positives:", fp, "\n")
cat("False Negatives:", fn, "\n")
cat("True Negatives:", tn, "\n")




```

```{r}
# Install and load the pROC package
library(pROC)

# Fit Logistic Regression Model
LogModel <- glm(Attrition_Flag ~ ., family = "binomial", data = training_reg)
print(summary(LogModel))
anova(LogModel, test = "Chisq")

# Make predictions on the testing set
log_reg <- predict(LogModel, testing_reg[-1], type = "response")
threshold <- 0.7  # Adjust the threshold as needed
y_pred <- ifelse(log_reg > threshold, 2, 1)
y_pred <- as.numeric(y_pred)
target <- as.numeric(testing_reg$Attrition_Flag)
target <- as.numeric(testing_reg$Attrition_Flag)

# Confusion Matrix and Accuracy
conf_matrix <- caret::confusionMatrix(table(y_pred, target))
log_accuracy <- conf_matrix$overall["Accuracy"]
cat("Accuracy:", log_accuracy, "\n")

# Print confusion matrix
cat("Confusion Matrix:\n")
print(conf_matrix$table)
# Calculate TP, TN, FP, FN
TP <- conf_matrix$table[2, 2]
TN <- conf_matrix$table[1, 1]
FP <- conf_matrix$table[1, 2]
FN <- conf_matrix$table[2, 1]




```

```{r}
# Install the package if it's not already installed
if (!require(caTools)) {
  install.packages("caTools")
}

# Load the package
library(caTools)

# Now you can use the sample.split function
split <- sample.split(data$Attrition_Flag, SplitRatio = 0.8)
train_df <- subset(data, split == TRUE)
test_df  <- subset(data, split == FALSE)

# Now, you can separate the predictors (X) from the target (y)
X_train <- train_df[, -which(names(train_df) %in% "Attrition_Flag")]
y_train <- train_df$Attrition_Flag
X_test <- test_df[, -which(names(test_df) %in% "Attrition_Flag")]
y_test <- test_df$Attrition_Flag
```





```{r}
# Create a copy of the dataframes
X_test_copy <- X_test
y_test_copy <- y_test
```


```{r}
head(X_test)
```

```{r}
colnames(train_df)
```

```{r}
# Select predictors (X) excluding 'CLIENTNUM' and 'Attrition_Flag'
X <- train_df[, !(names(train_df) %in% c('CLIENTNUM', 'Attrition_Flag'))]

# Select target variable (y)
y <- train_df$Attrition_Flag
```

```{r, onhot}

#Applying one hot encoding for data to be used in logistics regression or knn 
X_1 <- X

for (i in c('Gender', 'Education_Level', 'Marital_Status', 'Income_Category', 'Card_Category')) {
  tempdf <- model.matrix(~ . - 1, data = data.frame(X_1[i]))
  colnames(tempdf) <- gsub(sprintf("^%s", i), "", colnames(tempdf))
  X_1 <- cbind(X_1, tempdf)
  X_1 <- X_1[, !grepl(i, colnames(X_1))]
}

head(X_1)

```

```{r}
# Print the first few rows of the modified dataframe
head(X_1)
```

```{r, summary}
library(psych)
describe(X_1)
```

```{r}
# Create a copy of the dataframe
X_2 <- X_1

# Define the numeric columns to be scaled
num_cols <- c('Customer_Age', 'Dependent_count', 'Months_on_book', 
              'Total_Relationship_Count', 'Months_Inactive_12_mon', 
              'Contacts_Count_12_mon', 'Credit_Limit', 'Total_Revolving_Bal', 
              'Avg_Open_To_Buy', 'Total_Amt_Chng_Q4_Q1')

# Perform feature scaling
X_2[num_cols] <- scale(X_2[num_cols])

# Print the modified dataframe
print(X_2)
```

```{r, VIF}
# Assuming X_2 is your dataframe
model <- lm(Credit_Limit ~ ., data = X_2)  # Replace 'y' with your response variable
summary(model)

# Identify aliased coefficients (if any)
aliased_coefs <- which(is.na(coef(model)))
if (length(aliased_coefs) > 0) {
  aliased_vars <- names(coef(model))[aliased_coefs]
  print(paste("Aliased variables:", paste(aliased_vars, collapse = ", ")))
} else {
  print("No aliased variables found.")
}


```

```{r , Vif_continued}


# Define a function to calculate VIF
vif_df <- function(X_1) {
  vif_values <- data.frame(Feature = colnames(X_1), VIF_Factor = numeric(ncol(X_1)))
  
  for (i in seq_along(colnames(X_1))) {
    formula_str <- paste(colnames(X_1)[i], "~ .")
    lm_model <- lm(as.formula(formula_str), data = X_1)
    
    # Check for aliased coefficients
    if (!is.null(aliased_coefs <- which(is.na(coef(lm_model))))) {
      aliased_vars <- names(coef(lm_model))[aliased_coefs]
      cat("Aliased variables in model:", paste(aliased_vars, collapse = ", "), "\n")
    } else {
      vif_values$VIF_Factor[i] <- car::vif(lm_model)
    }
  }
  
  return(vif_values)
}

# Call the function with your data frame X_1
result <- vif_df(X_1)

# Print the result
print(result)
vif <- vif_df(X_2[num_cols])

# Print the result
print(vif)



```



```{r}
vif_cols <- c('Credit_Limit', 'Total_Revolving_Bal', 'Avg_Open_To_Buy')
X_2 <- X_2[, !(names(X_2) %in% vif_cols)]
num_cols <- num_cols[!(num_cols %in% vif_cols)]
X_1 <- X_1[, !(names(X_1) %in% vif_cols)]



```


```{r}

# Install and load the 'ROSE' package if not already installed
# install.packages("ROSE")
library(ROSE)

# Assuming 'X_1' is your predictor matrix and 'y' is your response variable
# Combine 'X_1' and 'y' into a data frame
df <- data.frame(cbind(y, X_1))

# Perform random oversampling
oversampled_data <- ovun.sample(y ~ ., data = df, method = "over", p = 0.4, seed = 123)

# Extract oversampled data
X_1_oversampled <- oversampled_data$data[, -1]  # Exclude the response variable
y_oversampled <- oversampled_data$data$y

# Now, 'X_1_oversampled' and 'y_oversampled' contain the randomly oversampled data


```




```{r}
# Assuming 'X_1' is your data frame and 'num_cols' is a vector of numeric column names
# Install and load the 'caret' package if not already installed
# install.packages("caret")
library(caret)

# Extract numeric columns for standardization
numeric_cols <- X_1[, num_cols]

# Standardize numeric columns using the scale function
scaled_numeric_cols <- scale(numeric_cols)

# Replace the original numeric columns with the standardized values
X_1[, num_cols] <- scaled_numeric_cols

# Print the modified data frame
print(X_1)


```


```{r}
library(pROC)

# Fit Logistic Regression Model
LogModel <- glm(Attrition_Flag ~ ., family = "binomial", data = training_reg)
print(summary(LogModel))
anova(LogModel, test = "Chisq")
# Make predictions on the testing set
log_reg <- predict(LogModel, testing_reg[-1], type = "response")
threshold <- 0.7  # Adjust the threshold as needed
y_pred <- ifelse(log_reg > threshold, 2, 1)
y_pred <- as.numeric(y_pred)
target <- as.numeric(testing_reg$Attrition_Flag)





```


```{r}


X_test[, num_cols] <- scale(X_test[, num_cols])

# Map target categories in 'y_test' to numerical values using 'target_dict'
# Replace 'target_dict' with the actual mapping based on your data
# Create a new factor with desired levels and labels
# Assuming 'y_test' is your response variable

# Convert 'y_test' to a factor with levels 0 and 1
y_test <- factor(y_test, levels = c("Existing Customer", "Attrited Customer"))

# Map levels to 0 and 1
y_test <- as.numeric(y_test) - 1

# Print the modified response variable
print(y_test)




```




```{r}
# Assuming 'X_test' is your data frame
# Assuming 'vif_cols' is a vector of column names to be dropped

# Drop columns specified in 'vif_cols'
X_test <- X_test[, !(names(X_test) %in% vif_cols)]

# Print the modified data frame
print(X_test)


```
```{r}
# Assuming 'lr_model' is your logistic regression model
# Assuming 'X_test' and 'X_1' are your test and training data frames
names(X_test)[names(X_test) == "High School"] <- "high_school"
names(X_test)[names(X_test) == "Post-Graduate"] <- "Post_Graduate"
names(X_test)[names(X_test) == "$120K +"] <- "X120K"
names(X_test)[names(X_test) == "$40K - $60K"] <- "X40K..X60K."
names(X_test)[names(X_test) == "$60K - $80K"] <- "X60K..X80K."
names(X_test)[names(X_test) == "$80K - $120K"] <- "X80K..X120K."
names(X_test)[names(X_test) == "Less than $40K"] <- "Less_than_X40K."

# Predict on the test set
y_test_pred <- predict(lr_model, newdata = X_test, type = "response")

# Predict on the training set
y_train_pred <- predict(lr_model, newdata = X_1, type = "response")

# Convert probabilities to binary predictions (0 or 1)
threshold <- 0.5  # Adjust the threshold as needed
y_test_pred_binary <- as.numeric(y_test_pred > threshold)
y_train_pred_binary <- as.numeric(y_train_pred > threshold)

# Print the binary predictions
print(y_test_pred_binary)
print(y_train_pred_binary)



```

```{r}
y_train_pred_binary

```


```{r}
# Install and load the necessary packages if not already installed
# install.packages("caret")
#install.packages("yardstick")
library(caret)
library(yardstick)

# Assuming 'y' is your true labels and 'y_train_pred_binary' is your predicted labels for the training set
# Convert 'y_train_pred_binary' to a factor with levels matching 'y'
y_test<-as.factor(y_test)
y_test_pred_binary <- as.factor(y_test_pred_binary)
levels(y_test_pred_binary) <- levels(y_test)

# Calculate accuracy for the training set
accuracy_value_train <- confusionMatrix(data = y_test_pred_binary, reference = y_test)$overall["Accuracy"]
print(paste("The accuracy of the model on the training set is:", accuracy_value_train))

# Print the classification report for the training set
conf_matrix_train <- confusionMatrix(data = y_test_pred_binary, reference = y_test)
classification_report_train <- confusionMatrix:::print.confusionMatrix(conf_matrix_train, digits = 3)
print("-----------The classification report for the training set is ----------")
print(classification_report_train)




```
```{r}
length(y_test_pred_binary)
length(y_test)
```






