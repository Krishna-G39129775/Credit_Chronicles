# Replace 'target_dict' with the actual mapping based on your data
# Install and load the 'dplyr' package if not already installed
# install.packages("dplyr")
library(dplyr)
# Assuming 'y' is your response variable
# Assuming 'target_dict' is a named vector or a data frame mapping target categories to numerical values
# Create a copy of X_test
X_test_tree <- X_test
# Drop the 'CLIENTNUM' column
X_test <- X_test[, !colnames(X_test) %in% c('CLIENTNUM')]
# One-hot encode categorical variables
for (i in c('Gender', 'Education_Level', 'Marital_Status', 'Income_Category', 'Card_Category')) {
tempdf <- model.matrix(~ . - 1, data = data.frame(X_test[i]))
colnames(tempdf) <- gsub(sprintf("^%s", i), "", colnames(tempdf))
X_test <- cbind(X_test, tempdf)
X_test <- X_test[, !grepl(i, colnames(X_test))]
}
# Standardize numeric columns
X_test[, num_cols] <- scale(X_test[, num_cols])
# Map target categories in 'y_test' to numerical values using 'target_dict'
# Replace 'target_dict' with the actual mapping based on your data
# Create a new factor with desired levels and labels
y_test <- as.character(y_test)
# Map "Existing Customer" to 0 and "Attrited Customer" to 1
y_test <- ifelse(y_test == "Existing Customer", 0, 1)
# Print the modified data frames
print(X_test)
print(y_test)
as.numeric(as.factor(y_test))
library(caret)
library(nnet)
library(rpart)
library(randomForest)
library(gbm)
library(e1071)
library(pROC)
# Load the data
data <- read.csv('credit_card.csv')
# Drop specific columns
data$Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_1 <- NULL
data$Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_2 <- NULL
# Install the package if it's not already installed
if (!require(caTools)) {
install.packages("caTools")
}
# Load the package
library(caTools)
# Now you can use the sample.split function
split <- sample.split(data$Attrition_Flag, SplitRatio = 0.8)
train_df <- subset(data, split == TRUE)
test_df  <- subset(data, split == FALSE)
# Now, you can separate the predictors (X) from the target (y)
X_train <- train_df[, -which(names(train_df) %in% "Attrition_Flag")]
y_train <- train_df$Attrition_Flag
X_test <- test_df[, -which(names(test_df) %in% "Attrition_Flag")]
y_test <- test_df$Attrition_Flag
# Create a copy of the dataframes
X_test_copy <- X_test
y_test_copy <- y_test
head(X_test)
colnames(train_df)
# Select predictors (X) excluding 'CLIENTNUM' and 'Attrition_Flag'
X <- train_df[, !(names(train_df) %in% c('CLIENTNUM', 'Attrition_Flag'))]
# Select target variable (y)
y <- train_df$Attrition_Flag
#Applying one hot encoding for data to be used in logistics regression or knn
X_1 <- X
for (i in c('Gender', 'Education_Level', 'Marital_Status', 'Income_Category', 'Card_Category')) {
tempdf <- model.matrix(~ . - 1, data = data.frame(X_1[i]))
colnames(tempdf) <- gsub(sprintf("^%s", i), "", colnames(tempdf))
X_1 <- cbind(X_1, tempdf)
X_1 <- X_1[, !grepl(i, colnames(X_1))]
}
head(X_1)
# Print the first few rows of the modified dataframe
head(X_1)
library(psych)
describe(X_1)
# Create a copy of the dataframe
X_2 <- X_1
# Define the numeric columns to be scaled
num_cols <- c('Customer_Age', 'Dependent_count', 'Months_on_book',
'Total_Relationship_Count', 'Months_Inactive_12_mon',
'Contacts_Count_12_mon', 'Credit_Limit', 'Total_Revolving_Bal',
'Avg_Open_To_Buy', 'Total_Amt_Chng_Q4_Q1')
# Perform feature scaling
X_2[num_cols] <- scale(X_2[num_cols])
# Print the modified dataframe
print(X_2)
# Assuming X_2 is your dataframe
model <- lm(Credit_Limit ~ ., data = X_2)  # Replace 'y' with your response variable
summary(model)
# Identify aliased coefficients (if any)
aliased_coefs <- which(is.na(coef(model)))
if (length(aliased_coefs) > 0) {
aliased_vars <- names(coef(model))[aliased_coefs]
print(paste("Aliased variables:", paste(aliased_vars, collapse = ", ")))
} else {
print("No aliased variables found.")
}
# Install and load the 'car' package if not already installed
# install.packages("car")
names(X_1)[names(X_1) == "High School"] <- "high_school"
names(X_1)[names(X_1) == "Post-Graduate"] <- "Post_Graduate"
names(X_1)[names(X_1) == "$120K +"] <- "X120K"
names(X_1)[names(X_1) == "$40K - $60K"] <- "X40K..X60K."
names(X_1)[names(X_1) == "$60K - $80K"] <- "X60K..X80K."
names(X_1)[names(X_1) == "$80K - $120K"] <- "X80K..X120K."
names(X_1)[names(X_1) == "Less than $40K"] <- "Less_than_X40K."
library(car)
# Define a function to calculate VIF
vif_df <- function(X_1) {
vif_values <- data.frame(Feature = colnames(X_1), VIF_Factor = numeric(ncol(X_1)))
for (i in seq_along(colnames(X_1))) {
formula_str <- paste(colnames(X_1)[i], "~ .")
lm_model <- lm(as.formula(formula_str), data = X_1)
# Check for aliased coefficients
if (!is.null(aliased_coefs <- which(is.na(coef(lm_model))))) {
aliased_vars <- names(coef(lm_model))[aliased_coefs]
cat("Aliased variables in model:", paste(aliased_vars, collapse = ", "), "\n")
} else {
vif_values$VIF_Factor[i] <- car::vif(lm_model)
}
}
return(vif_values)
}
# Call the function with your data frame X_1
result <- vif_df(X_1)
# Print the result
print(result)
vif <- vif_df(X_2[num_cols])
# Print the result
print(vif)
#In above evry vif value is zero so i did thts way check it (the values are matching with the kaggle one)
model <- lm(Credit_Limit ~ ., data = X_2[num_cols])
ezids::xkablevif(model)
vif_cols <- c('Credit_Limit', 'Total_Revolving_Bal', 'Avg_Open_To_Buy')
X_2 <- X_2[, !(names(X_2) %in% vif_cols)]
num_cols <- num_cols[!(num_cols %in% vif_cols)]
X_1 <- X_1[, !(names(X_1) %in% vif_cols)]
# Install and load the 'ROSE' package if not already installed
# install.packages("ROSE")
library(ROSE)
# Assuming 'X_1' is your predictor matrix and 'y' is your response variable
# Combine 'X_1' and 'y' into a data frame
df <- data.frame(cbind(y, X_1))
# Perform random oversampling
oversampled_data <- ovun.sample(y ~ ., data = df, method = "over", p = 0.4, seed = 123)
# Extract oversampled data
X_1_oversampled <- oversampled_data$data[, -1]  # Exclude the response variable
y_oversampled <- oversampled_data$data$y
# Now, 'X_1_oversampled' and 'y_oversampled' contain the randomly oversampled data
# Assuming 'X_1' is your data frame and 'num_cols' is a vector of numeric column names
# Install and load the 'caret' package if not already installed
# install.packages("caret")
library(caret)
# Extract numeric columns for standardization
numeric_cols <- X_1[, num_cols]
# Standardize numeric columns using the scale function
scaled_numeric_cols <- scale(numeric_cols)
# Replace the original numeric columns with the standardized values
X_1[, num_cols] <- scaled_numeric_cols
# Print the modified data frame
print(X_1)
# Assuming 'y' is your response variable
# Check the unique values in 'y'
unique_values <- unique(y)
# If there are more than two unique values, handle them appropriately
if (length(unique_values) > 2) {
stop("Response variable 'y' has more than two unique values. Check the data.")
}
# Encode 'y' as 0 and 1
y <- as.numeric(y == "Attrited Customer")
# Fit logistic regression model
lr_model <- glm(y ~ ., family = binomial(link = "logit"), data = X_1)
# Print the summary of the logistic regression model
summary(lr_model)
# Assuming 'X_test' is your test data frame and 'num_cols' is a vector of numeric column names
# Assuming 'sc_scale' is a scaling function applied to 'num_cols'
# Assuming 'target_dict' is a dictionary mapping target categories in 'y_test' to numerical values
# Assuming 'y' is your response variable
# Assuming 'target_dict' is a named vector or a data frame mapping target categories to numerical values
# Map target categories in 'y' to numerical values using 'target_dict'
# Replace 'target_dict' with the actual mapping based on your data
# Install and load the 'dplyr' package if not already installed
# install.packages("dplyr")
library(dplyr)
# Assuming 'y' is your response variable
# Assuming 'target_dict' is a named vector or a data frame mapping target categories to numerical values
# Create a copy of X_test
X_test_tree <- X_test
# Drop the 'CLIENTNUM' column
X_test <- X_test[, !colnames(X_test) %in% c('CLIENTNUM')]
# One-hot encode categorical variables
for (i in c('Gender', 'Education_Level', 'Marital_Status', 'Income_Category', 'Card_Category')) {
tempdf <- model.matrix(~ . - 1, data = data.frame(X_test[i]))
colnames(tempdf) <- gsub(sprintf("^%s", i), "", colnames(tempdf))
X_test <- cbind(X_test, tempdf)
X_test <- X_test[, !grepl(i, colnames(X_test))]
}
# Standardize numeric columns
X_test[, num_cols] <- scale(X_test[, num_cols])
# Map target categories in 'y_test' to numerical values using 'target_dict'
# Replace 'target_dict' with the actual mapping based on your data
# Create a new factor with desired levels and labels
# Assuming 'y_test' is your response variable
# Convert 'y_test' to a factor with levels 0 and 1
y_test <- factor(y_test, levels = c("Existing Customer", "Attrited Customer"))
# Map levels to 0 and 1
y_test <- as.numeric(y_test) - 1
# Print the modified response variable
print(y_test)
# Assuming 'X_test' is your data frame
# Assuming 'vif_cols' is a vector of column names to be dropped
# Drop columns specified in 'vif_cols'
X_test <- X_test[, !(names(X_test) %in% vif_cols)]
# Print the modified data frame
print(X_test)
# Assuming 'lr_model' is your logistic regression model
# Assuming 'X_test' and 'X_1' are your test and training data frames
names(X_test)[names(X_test) == "High School"] <- "high_school"
names(X_test)[names(X_test) == "Post-Graduate"] <- "Post_Graduate"
names(X_test)[names(X_test) == "$120K +"] <- "X120K"
names(X_test)[names(X_test) == "$40K - $60K"] <- "X40K..X60K."
names(X_test)[names(X_test) == "$60K - $80K"] <- "X60K..X80K."
names(X_test)[names(X_test) == "$80K - $120K"] <- "X80K..X120K."
names(X_test)[names(X_Test) == "Less than $40K"] <- "Less_than_X40K."
library(caret)
library(nnet)
library(rpart)
library(randomForest)
library(gbm)
library(e1071)
library(pROC)
# Load the data
data <- read.csv('credit_card.csv')
# Drop specific columns
data$Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_1 <- NULL
data$Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_2 <- NULL
# Install the package if it's not already installed
if (!require(caTools)) {
install.packages("caTools")
}
# Load the package
library(caTools)
# Now you can use the sample.split function
split <- sample.split(data$Attrition_Flag, SplitRatio = 0.8)
train_df <- subset(data, split == TRUE)
test_df  <- subset(data, split == FALSE)
# Now, you can separate the predictors (X) from the target (y)
X_train <- train_df[, -which(names(train_df) %in% "Attrition_Flag")]
y_train <- train_df$Attrition_Flag
X_test <- test_df[, -which(names(test_df) %in% "Attrition_Flag")]
y_test <- test_df$Attrition_Flag
# Create a copy of the dataframes
X_test_copy <- X_test
y_test_copy <- y_test
head(X_test)
colnames(train_df)
# Select predictors (X) excluding 'CLIENTNUM' and 'Attrition_Flag'
X <- train_df[, !(names(train_df) %in% c('CLIENTNUM', 'Attrition_Flag'))]
# Select target variable (y)
y <- train_df$Attrition_Flag
#Applying one hot encoding for data to be used in logistics regression or knn
X_1 <- X
for (i in c('Gender', 'Education_Level', 'Marital_Status', 'Income_Category', 'Card_Category')) {
tempdf <- model.matrix(~ . - 1, data = data.frame(X_1[i]))
colnames(tempdf) <- gsub(sprintf("^%s", i), "", colnames(tempdf))
X_1 <- cbind(X_1, tempdf)
X_1 <- X_1[, !grepl(i, colnames(X_1))]
}
head(X_1)
# Print the first few rows of the modified dataframe
head(X_1)
library(psych)
describe(X_1)
# Create a copy of the dataframe
X_2 <- X_1
# Define the numeric columns to be scaled
num_cols <- c('Customer_Age', 'Dependent_count', 'Months_on_book',
'Total_Relationship_Count', 'Months_Inactive_12_mon',
'Contacts_Count_12_mon', 'Credit_Limit', 'Total_Revolving_Bal',
'Avg_Open_To_Buy', 'Total_Amt_Chng_Q4_Q1')
# Perform feature scaling
X_2[num_cols] <- scale(X_2[num_cols])
# Print the modified dataframe
print(X_2)
# Assuming X_2 is your dataframe
model <- lm(Credit_Limit ~ ., data = X_2)  # Replace 'y' with your response variable
summary(model)
# Identify aliased coefficients (if any)
aliased_coefs <- which(is.na(coef(model)))
if (length(aliased_coefs) > 0) {
aliased_vars <- names(coef(model))[aliased_coefs]
print(paste("Aliased variables:", paste(aliased_vars, collapse = ", ")))
} else {
print("No aliased variables found.")
}
# Install and load the 'car' package if not already installed
# install.packages("car")
names(X_1)[names(X_1) == "High School"] <- "high_school"
names(X_1)[names(X_1) == "Post-Graduate"] <- "Post_Graduate"
names(X_1)[names(X_1) == "$120K +"] <- "X120K"
names(X_1)[names(X_1) == "$40K - $60K"] <- "X40K..X60K."
names(X_1)[names(X_1) == "$60K - $80K"] <- "X60K..X80K."
names(X_1)[names(X_1) == "$80K - $120K"] <- "X80K..X120K."
names(X_1)[names(X_1) == "Less than $40K"] <- "Less_than_X40K."
library(car)
# Define a function to calculate VIF
vif_df <- function(X_1) {
vif_values <- data.frame(Feature = colnames(X_1), VIF_Factor = numeric(ncol(X_1)))
for (i in seq_along(colnames(X_1))) {
formula_str <- paste(colnames(X_1)[i], "~ .")
lm_model <- lm(as.formula(formula_str), data = X_1)
# Check for aliased coefficients
if (!is.null(aliased_coefs <- which(is.na(coef(lm_model))))) {
aliased_vars <- names(coef(lm_model))[aliased_coefs]
cat("Aliased variables in model:", paste(aliased_vars, collapse = ", "), "\n")
} else {
vif_values$VIF_Factor[i] <- car::vif(lm_model)
}
}
return(vif_values)
}
# Call the function with your data frame X_1
result <- vif_df(X_1)
# Print the result
print(result)
vif <- vif_df(X_2[num_cols])
# Print the result
print(vif)
#In above evry vif value is zero so i did thts way check it (the values are matching with the kaggle one)
model <- lm(Credit_Limit ~ ., data = X_2[num_cols])
ezids::xkablevif(model)
vif_cols <- c('Credit_Limit', 'Total_Revolving_Bal', 'Avg_Open_To_Buy')
X_2 <- X_2[, !(names(X_2) %in% vif_cols)]
num_cols <- num_cols[!(num_cols %in% vif_cols)]
X_1 <- X_1[, !(names(X_1) %in% vif_cols)]
# Install and load the 'ROSE' package if not already installed
# install.packages("ROSE")
library(ROSE)
# Assuming 'X_1' is your predictor matrix and 'y' is your response variable
# Combine 'X_1' and 'y' into a data frame
df <- data.frame(cbind(y, X_1))
# Perform random oversampling
oversampled_data <- ovun.sample(y ~ ., data = df, method = "over", p = 0.4, seed = 123)
# Extract oversampled data
X_1_oversampled <- oversampled_data$data[, -1]  # Exclude the response variable
y_oversampled <- oversampled_data$data$y
# Now, 'X_1_oversampled' and 'y_oversampled' contain the randomly oversampled data
# Assuming 'X_1' is your data frame and 'num_cols' is a vector of numeric column names
# Install and load the 'caret' package if not already installed
# install.packages("caret")
library(caret)
# Extract numeric columns for standardization
numeric_cols <- X_1[, num_cols]
# Standardize numeric columns using the scale function
scaled_numeric_cols <- scale(numeric_cols)
# Replace the original numeric columns with the standardized values
X_1[, num_cols] <- scaled_numeric_cols
# Print the modified data frame
print(X_1)
# Assuming 'y' is your response variable
# Check the unique values in 'y'
unique_values <- unique(y)
# If there are more than two unique values, handle them appropriately
if (length(unique_values) > 2) {
stop("Response variable 'y' has more than two unique values. Check the data.")
}
# Encode 'y' as 0 and 1
y <- as.numeric(y == "Attrited Customer")
# Fit logistic regression model
lr_model <- glm(y ~ ., family = binomial(link = "logit"), data = X_1)
# Print the summary of the logistic regression model
summary(lr_model)
# Assuming 'X_test' is your test data frame and 'num_cols' is a vector of numeric column names
# Assuming 'sc_scale' is a scaling function applied to 'num_cols'
# Assuming 'target_dict' is a dictionary mapping target categories in 'y_test' to numerical values
# Assuming 'y' is your response variable
# Assuming 'target_dict' is a named vector or a data frame mapping target categories to numerical values
# Map target categories in 'y' to numerical values using 'target_dict'
# Replace 'target_dict' with the actual mapping based on your data
# Install and load the 'dplyr' package if not already installed
# install.packages("dplyr")
library(dplyr)
# Assuming 'y' is your response variable
# Assuming 'target_dict' is a named vector or a data frame mapping target categories to numerical values
# Create a copy of X_test
X_test_tree <- X_test
# Drop the 'CLIENTNUM' column
X_test <- X_test[, !colnames(X_test) %in% c('CLIENTNUM')]
# One-hot encode categorical variables
for (i in c('Gender', 'Education_Level', 'Marital_Status', 'Income_Category', 'Card_Category')) {
tempdf <- model.matrix(~ . - 1, data = data.frame(X_test[i]))
colnames(tempdf) <- gsub(sprintf("^%s", i), "", colnames(tempdf))
X_test <- cbind(X_test, tempdf)
X_test <- X_test[, !grepl(i, colnames(X_test))]
}
# Standardize numeric columns
X_test[, num_cols] <- scale(X_test[, num_cols])
# Map target categories in 'y_test' to numerical values using 'target_dict'
# Replace 'target_dict' with the actual mapping based on your data
# Create a new factor with desired levels and labels
# Assuming 'y_test' is your response variable
# Convert 'y_test' to a factor with levels 0 and 1
y_test <- factor(y_test, levels = c("Existing Customer", "Attrited Customer"))
# Map levels to 0 and 1
y_test <- as.numeric(y_test) - 1
# Print the modified response variable
print(y_test)
levels(y_test)
str(y_test)
as.factor(y_test)
levels(y_test)
as.factor(y_test)
levels(y_test)
# Assuming 'X_test' is your data frame
# Assuming 'vif_cols' is a vector of column names to be dropped
# Drop columns specified in 'vif_cols'
X_test <- X_test[, !(names(X_test) %in% vif_cols)]
# Print the modified data frame
print(X_test)
# Assuming 'lr_model' is your logistic regression model
# Assuming 'X_test' and 'X_1' are your test and training data frames
names(X_test)[names(X_test) == "High School"] <- "high_school"
names(X_test)[names(X_test) == "Post-Graduate"] <- "Post_Graduate"
names(X_test)[names(X_test) == "$120K +"] <- "X120K"
names(X_test)[names(X_test) == "$40K - $60K"] <- "X40K..X60K."
names(X_test)[names(X_test) == "$60K - $80K"] <- "X60K..X80K."
names(X_test)[names(X_test) == "$80K - $120K"] <- "X80K..X120K."
names(X_test)[names(X_Test) == "Less than $40K"] <- "Less_than_X40K."
# Assuming 'lr_model' is your logistic regression model
# Assuming 'X_test' and 'X_1' are your test and training data frames
names(X_test)[names(X_test) == "High School"] <- "high_school"
names(X_test)[names(X_test) == "Post-Graduate"] <- "Post_Graduate"
names(X_test)[names(X_test) == "$120K +"] <- "X120K"
names(X_test)[names(X_test) == "$40K - $60K"] <- "X40K..X60K."
names(X_test)[names(X_test) == "$60K - $80K"] <- "X60K..X80K."
names(X_test)[names(X_test) == "$80K - $120K"] <- "X80K..X120K."
names(X_test)[names(X_test) == "Less than $40K"] <- "Less_than_X40K."
# Predict on the test set
y_test_pred <- predict(lr_model, newdata = X_test, type = "response")
# Predict on the training set
y_train_pred <- predict(lr_model, newdata = X_1, type = "response")
# Convert probabilities to binary predictions (0 or 1)
threshold <- 0.5  # Adjust the threshold as needed
y_test_pred_binary <- as.numeric(y_test_pred > threshold)
y_train_pred_binary <- as.numeric(y_train_pred > threshold)
# Print the binary predictions
print(y_test_pred_binary)
print(y_train_pred_binary)
# Install and load the necessary packages if not already installed
# install.packages("caret")
#install.packages("yardstick")
library(caret)
library(yardstick)
# Assuming 'y' is your true labels and 'y_train_pred_binary' is your predicted labels for the training set
# Convert 'y_train_pred_binary' to a factor with levels matching 'y'
y_test<-as.factor(y_test)
y_train_pred_binary <- as.factor(y_train_pred_binary)
levels(y_train_pred_binary) <- levels(y_test)
# Calculate accuracy for the training set
accuracy_value_train <- confusionMatrix(data = y_train_pred_binary, reference = y_test)$overall["Accuracy"]
# Install and load the necessary packages if not already installed
# install.packages("caret")
#install.packages("yardstick")
library(caret)
library(yardstick)
# Assuming 'y' is your true labels and 'y_train_pred_binary' is your predicted labels for the training set
# Convert 'y_train_pred_binary' to a factor with levels matching 'y'
y_test<-as.factor(y_test)
y_train_pred_binary <- as.factor(y_train_pred_binary)
levels(y_train_pred_binary) <- levels(y_test)
# Calculate accuracy for the training set
accuracy_value_train <- confusionMatrix(data = y_train_pred_binary, reference = y_train)$overall["Accuracy"]
# Install and load the necessary packages if not already installed
# install.packages("caret")
#install.packages("yardstick")
library(caret)
library(yardstick)
# Assuming 'y' is your true labels and 'y_train_pred_binary' is your predicted labels for the training set
# Convert 'y_train_pred_binary' to a factor with levels matching 'y'
y_test<-as.factor(y_train)
y_train_pred_binary <- as.factor(y_train_pred_binary)
levels(y_train_pred_binary) <- levels(y_train)
as.factor(y_train)
levels(y_train)
# Install and load the necessary packages if not already installed
# install.packages("caret")
#install.packages("yardstick")
library(caret)
library(yardstick)
# Assuming 'y' is your true labels and 'y_train_pred_binary' is your predicted labels for the training set
# Convert 'y_train_pred_binary' to a factor with levels matching 'y'
y_test<-as.factor(y_test)
y_test_pred_binary <- as.factor(y_test_pred_binary)
levels(y_test_pred_binary) <- levels(y_test)
# Calculate accuracy for the training set
accuracy_value_train <- confusionMatrix(data = y_test_pred_binary, reference = y_test)$overall["Accuracy"]
levels(y_test_pred_binary)
levels(y_test)
lenght(y_test_pred_binary)
length(y_test_pred_binary)
length(y_test)
Credit_card_churn <- read.csv('credit_card1.csv')
Credit_card_churn <- read.csv('C:/Users/goura/Downloads/Part-2.zip/Part-2credit_card1.csv')
Credit_card_churn <- read.csv('C:/Users/goura/Downloads/Part-2.zip/Part-2/credit_card1.csv')
# Load the data
data <- read.csv('credit_card.csv')
# Drop specific columns
data$Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_1 <- NULL
data$Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_2 <- NULL
