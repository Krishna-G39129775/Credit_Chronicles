---
title: "Team 8: Credit Chronicles "
author: "Members: Sai Krishna ,Gourab Mukherjee, Tulasi"
#date: "`r Sys.Date()`"
output:
  html_document:
    css: custom_theme.css
    code_folding: hide
---


```{r cars}
library(plyr)
library(corrplot)
library(ggplot2)
library(gridExtra)
library(caret)
library(MASS)
library(randomForest)
library(party)
library(rpart)
library(GoodmanKruskal)
library(rpart.plot)
```

```{r}
Credit_card_churn <- read.csv('credit_card1.csv')
str(Credit_card_churn)
```

```{r}
sapply(Credit_card_churn, function(x) sum(is.na(x)))

Credit_card_churn$Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_1  <- NULL
Credit_card_churn$Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_2 <- NULL
Credit_card_churn$CLIENTNUM <- NULL

```

```{r}
str(Credit_card_churn)
summary(Credit_card_churn)
```

```{r}
print(head(Credit_card_churn,5))
```



```{r}
table(Credit_card_churn$Attrition_Flag, Credit_card_churn$Customer_Age)
table(Credit_card_churn$Attrition_Flag, Credit_card_churn$Gender)

```

```{r}
ggplot(Credit_card_churn, aes(x=Attrition_Flag,
                  y= prop.table(stat(count)),
                  fill= factor(Gender),
                  label= scales::percent(prop.table(stat(count))))) +
  geom_bar(position = position_dodge())+
  geom_text(stat="count",
            position = position_dodge(.9),
            vjust= -0.5, size=3)+
  scale_y_continuous(labels = scales::percent)+
  labs(title = "Attrition by Gender",
       x= "Attrition status",
       y="Count")+
  theme_classic()+
  scale_fill_discrete(
    name="Gender",
    breaks=c("M", "F"),
    labels=c("Male", "Female" )
  )

ggplot(Credit_card_churn, aes(x=Attrition_Flag,
                  y= prop.table(stat(count)),
                  fill= factor(Card_Category),
                  label= scales::percent(prop.table(stat(count))))) +
  geom_bar(position = position_dodge())+
  geom_text(stat="count",
            position = position_dodge(.9),
            vjust= -0.5, size=3)+
  scale_y_continuous(labels = scales::percent)+
  labs(title = "Attrition by Card Category",
       x= "Attrition status",
       y="Count")+
  theme_classic()

ggplot(Credit_card_churn, aes(x=Attrition_Flag,
                  y= prop.table(stat(count)),
                  fill= factor(Income_Category),
                  label= scales::percent(prop.table(stat(count))))) +
  geom_bar(position = position_dodge())+
  geom_text(stat="count",
            position = position_dodge(.9),
            vjust= -0.5, size=3)+
  scale_y_continuous(labels = scales::percent)+
  labs(title = "Attrition by Income Category",
       x= "Attrition status",
       y="Count")+
  theme_classic()


```



```{r}
#Converting all features to categorical data
Credit_card_churn[sapply(Credit_card_churn, is.character)]<- lapply(Credit_card_churn[sapply(Credit_card_churn, is.character)], as.factor)
```

```{r}
str(Credit_card_churn)
summary(Credit_card_churn)
```








```{r}
#Splitting the regular dataset

# Assuming you have loaded the necessary data as 'Credit_card_churn'

# Convert all features to categorical data
Credit_card_churn[sapply(Credit_card_churn, is.character)] <- lapply(Credit_card_churn[sapply(Credit_card_churn, is.character)], as.factor)

# Split the dataset into training and testing sets
set.seed(123)
intrain_reg <- createDataPartition(Credit_card_churn$Attrition_Flag, p = 0.80, list = FALSE)
training_reg <- Credit_card_churn[intrain_reg, ]
testing_reg <- Credit_card_churn[-intrain_reg, ]
dim(training_reg); dim(testing_reg)
#summary(training_reg)
#summary(testing_reg)
```



```{r}
# Fit Random Forest model
random_forest <- randomForest(Attrition_Flag ~ ., ntree = 500, family = "binomial", data = training_reg)

# Print summary of the random forest model
print(summary(random_forest))
print(random_forest)

# Make predictions on the testing set
rf_pred <- predict(random_forest, testing_reg)

# Calculate confusion matrix
conf_matrix_rf <- caret::confusionMatrix(rf_pred, testing_reg$Attrition_Flag)

# Extract overall accuracy from confusion matrix
ran_accuracy <- conf_matrix_rf$overall["Accuracy"]
cat("Random Forest Accuracy:", ran_accuracy, "\n")

# Print precision, recall, TP, FP, FN, TN
precision <- conf_matrix_rf$byClass["Precision"]
recall <- conf_matrix_rf$byClass["Recall"]
f1_score <- 2 * (precision * recall) / (precision + recall)  # Calculate F1 Score
tp <- conf_matrix_rf$table[2, 2]
fp <- conf_matrix_rf$table[1, 2]
fn <- conf_matrix_rf$table[2, 1]
tn <- conf_matrix_rf$table[1, 1]

cat("Precision:", precision, "\n")
cat("Recall:", recall, "\n")
cat("F1 Score:", f1_score, "\n")  # Print F1 Score
cat("True Positives:", tp, "\n")
cat("False Positives:", fp, "\n")
cat("False Negatives:", fn, "\n")
cat("True Negatives:", tn, "\n")




```



```{r}
# Install and load the pROC package
library(pROC)

# Fit Logistic Regression Model
LogModel <- glm(Attrition_Flag ~ ., family = "binomial", data = training_reg)
print(summary(LogModel))
anova(LogModel, test = "Chisq")

# Make predictions on the testing set
log_reg <- predict(LogModel, testing_reg[-1], type = "response")
threshold <- 0.7  # Adjust the threshold as needed
y_pred <- ifelse(log_reg > threshold, 2, 1)
y_pred <- as.numeric(y_pred)
target <- as.numeric(testing_reg$Attrition_Flag)

# Confusion Matrix and Accuracy
conf_matrix <- caret::confusionMatrix(table(y_pred, target))
log_accuracy <- conf_matrix$overall["Accuracy"]
cat("Accuracy:", log_accuracy, "\n")

# Print confusion matrix
cat("Confusion Matrix:\n")
print(conf_matrix$table)

# Calculate TP, TN, FP, FN
TP <- conf_matrix$table[2, 2]
TN <- conf_matrix$table[1, 1]
FP <- conf_matrix$table[1, 2]
FN <- conf_matrix$table[2, 1]

cat("\nTrue Positives (TP):", TP, "\n")
cat("True Negatives (TN):", TN, "\n")
cat("False Positives (FP):", FP, "\n")
cat("False Negatives (FN):", FN, "\n")

# Calculate Precision, Recall, and F1 Score
precision <- TP / (TP + FP)
recall <- TP / (TP + FN)
f1_score <- 2 * (precision * recall) / (precision + recall)

cat("\nPrecision:", precision, "\n")
cat("Recall (Sensitivity):", recall, "\n")
cat("F1 Score:", f1_score, "\n")

# ROC Curve and AUC
roc_curve <- roc(target, log_reg)
auc_value <- auc(roc_curve)

# Plot ROC Curve
plot(roc_curve, main = "ROC Curve", col = "blue", lwd = 2)
abline(a = 0, b = 1, lty = 2, col = "red")  # Diagonal line for reference

# Display AUC value
cat("AUC:", auc_value, "\n")

# Calculate accuracy from confusion matrix
accuracy_from_conf_matrix <- sum(diag(conf_matrix$table)) / sum(conf_matrix$table)
cat("Accuracy (calculated from Confusion Matrix):", accuracy_from_conf_matrix, "\n")


```


```{r}
# Decision Tree for Regular Data
decision_tree <- ctree(Attrition_Flag ~ ., data = training_reg)
print(decision_tree)

dt_pred <- predict(decision_tree, testing_reg)
conf_matrix_dt <- caret::confusionMatrix(dt_pred, testing_reg$Attrition_Flag)

# Extracting TP, TN, FP, FN
TP <- conf_matrix_dt$table[2, 2]  # True Positives
TN <- conf_matrix_dt$table[1, 1]  # True Negatives
FP <- conf_matrix_dt$table[1, 2]  # False Positives
FN <- conf_matrix_dt$table[2, 1]  # False Negatives

# Print TP, TN, FP, FN
cat("True Positives (TP):", TP, "\n")
cat("True Negatives (TN):", TN, "\n")
cat("False Positives (FP):", FP, "\n")
cat("False Negatives (FN):", FN, "\n")

# Calculate and print Precision, Recall, and F1 Score
precision <- conf_matrix_dt$byClass["Precision"]
recall <- conf_matrix_dt$byClass["Sensitivity"]  # Recall is also known as Sensitivity
f1_score <- 2 * (precision * recall) / (precision + recall)

cat("Precision:", precision, "\n")
cat("Recall (Sensitivity):", recall, "\n")
cat("F1 Score:", f1_score, "\n")

# Calculate and print accuracy
dec_accuracy <- conf_matrix_dt$overall["Accuracy"]
cat("Decision Tree Accuracy:", dec_accuracy, "\n")


```


```{r}
# Create a data frame for comparison
comparison_data <- data.frame(
  Algorithm = c("Random Forest", "Decision Tree", "Logistic Regression"),
  Percentage = c(ran_accuracy * 100, dec_accuracy * 100, accuracy_from_conf_matrix * 100)
)

# Load necessary library
library(ggplot2)

# Plot the comparison using ggplot with different colors and decreased width
ggplot(data = comparison_data, aes(x = Algorithm, y = Percentage, fill = Algorithm)) +
  geom_bar(stat = "identity", position = position_dodge(width = 0.7), width = 0.6) +
  scale_fill_brewer(palette = "Set3") +
  geom_text(aes(label = sprintf("%.2f%%", Percentage)), vjust = -0.2, size = 5, position = position_dodge(0.7)) +
  ylim(0, max(comparison_data$Percentage) * 1.1) +
  labs(title = "Comparison of Model Accuracy",
       y = "Accuracy (%)",
       x = "Algorithm")

```

